import requests
from bs4 import BeautifulSoup
import json
import time
import urllib3

# ç¦ç”¨SSLè­¦å‘Šï¼ˆå¯é€‰ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


def page_item_links(base_search_url, total_pages=92, museum_name="freer"):
    all_item_urls = []

    # é…ç½®é‡è¯•ç­–ç•¥
    session = requests.Session()
    retries = urllib3.util.retry.Retry(
        total=5,
        backoff_factor=0.3,
        status_forcelist=[500, 502, 503, 504]
    )
    session.mount(
        'https://', requests.adapters.HTTPAdapter(max_retries=retries))

    for page_num in range(0, total_pages + 1):
        url = base_search_url.format(page_num)
        try:
            # è§£å†³æ–¹æ¡ˆ1ï¼šç¦ç”¨SSLéªŒè¯ï¼ˆå¿«é€Ÿè§£å†³æ–¹æ³•ï¼‰
            response = session.get(
                url,
                headers=headers,
                verify=False,  # å…³é—­SSLéªŒè¯
                timeout=30
            )

            # è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç³»ç»Ÿè¯ä¹¦ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            # response = session.get(
            #     url,
            #     headers=headers,
            #     verify='/path/to/cert.pem'  # æˆ–ä½¿ç”¨certifi.where()
            # )

            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ä¼˜åŒ–é€‰æ‹©å™¨
            list_items = soup.find_all(
                'li', class_='search-results-image-grid__result-container')
            if not list_items:
                item_divs = soup.find_all(
                    'div', class_='search-results-image-grid__result')
            else:
                item_divs = [
                    li.find('div', class_='search-results-image-grid__result') for li in list_items]

            for item_div in item_divs:
                if item_div:  # é˜²æ­¢ç©ºå…ƒç´ 
                    a_tag = item_div.find('a', class_='secondary-link')
                    if a_tag and 'href' in a_tag.attrs:
                        full_url = f"https://asia.si.edu{a_tag['href']}"
                        all_item_urls.append(full_url)

            print(f"âœ… ç¬¬ {page_num} é¡µå®Œæˆï¼Œæ‰¾åˆ° {len(item_divs)} ä¸ªæ¡ç›®")
            time.sleep(0.3)  # å¢åŠ é—´éš”æ—¶é—´

        except Exception as e:
            print(f"âŒ ç¬¬ {page_num} é¡µå¤±è´¥: {str(e)}")
            continue

    # å»é‡ä¿å­˜
    unique_urls = list(set(all_item_urls))
    with open(f'{museum_name}_links.json', 'w', encoding='utf-8') as f:
        json.dump(unique_urls, f, ensure_ascii=False, indent=4)

    print(f"ğŸ‰ å®Œæˆï¼å…±æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€é“¾æ¥")


if __name__ == "__main__":
    museum_name = "freer_museum"
    base_search_url = "https://asia.si.edu/explore-art-culture/collections/search/?listStart={}&edan_fq[]=topic:Chinese+Art"

    # å»ºè®®å…ˆæµ‹è¯•å°‘é‡é¡µé¢
    page_item_links(
        base_search_url,
        total_pages=1168,  # å…ˆç”¨3é¡µæµ‹è¯•
        museum_name=museum_name
    )
